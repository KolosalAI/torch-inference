# Production Docker Compose override
# Use with: docker compose -f compose.yaml -f compose.prod.yaml up

services:
  torch-inference:
    deploy:
      replicas: 3
      resources:
        limits:
          memory: 6G
          cpus: '2'
        reservations:
          memory: 4G
          cpus: '1'
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first
        failure_action: rollback
      rollback_config:
        parallelism: 1
        delay: 5s
    environment:
      - ENVIRONMENT=production
      - LOG_LEVEL=WARNING
      - WORKERS=3
      - ENABLE_METRICS=true
      - ENABLE_PROFILING=false
      - GPU_MEMORY_FRACTION=0.9
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 120s

  # Production load balancer
  nginx:
    volumes:
      - ./nginx.prod.conf:/etc/nginx/nginx.conf:ro
      - nginx_cache:/var/cache/nginx
      - letsencrypt:/etc/letsencrypt:ro
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  # Production Redis with persistence
  redis:
    command: |
      redis-server 
      --appendonly yes 
      --appendfsync everysec 
      --maxmemory 2g 
      --maxmemory-policy allkeys-lru 
      --tcp-keepalive 60 
      --timeout 300
    deploy:
      resources:
        limits:
          memory: 2.5G
          cpus: '1'
        reservations:
          memory: 2G
          cpus: '0.5'

  # Production monitoring
  monitoring:
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 512M
          cpus: '0.25'

  # Log aggregation for production
  fluentd:
    image: fluentd:v1.16-1
    container_name: torch-inference-logs
    restart: unless-stopped
    volumes:
      - ./logging/fluentd.conf:/fluentd/etc/fluent.conf:ro
      - ./logs:/var/log/app:ro
    ports:
      - "24224:24224"
    networks:
      - inference_network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'

  # Production security scanner
  security-scanner:
    image: aquasec/trivy:latest
    container_name: torch-inference-security
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - trivy_cache:/root/.cache/trivy
    command: |
      sh -c "
        while true; do
          trivy image --exit-code 0 --severity HIGH,CRITICAL torch-inference_torch-inference:latest
          sleep 3600
        done
      "
    networks:
      - inference_network

volumes:
  nginx_cache:
    driver: local
  letsencrypt:
    driver: local
  trivy_cache:
    driver: local

# Production network with custom configuration
networks:
  inference_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
    driver_opts:
      com.docker.network.bridge.name: torch-inference-net
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.enable_ip_masquerade: "true"