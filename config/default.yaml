# Default Configuration for PyTorch Inference Framework

# Environment settings
debug: true
environment: "development"

# Server configuration
server:
  host: "0.0.0.0"
  port: 8000
  log_level: "INFO"
  reload: false
  workers: 1

# Security settings
security:
  max_file_size_mb: 100
  allowed_extensions:
    - ".wav"
    - ".mp3"
    - ".flac"
    - ".m4a"
    - ".ogg"
  enable_rate_limiting: true
  max_requests_per_minute: 100
  enable_cors: true
  allowed_origins:
    - "http://localhost:3000"
    - "http://localhost:8080"

# Audio processing settings
audio:
  default_tts_model: "speecht5_tts"
  default_stt_model: "whisper-base"
  max_text_length: 5000
  default_sample_rate: 16000
  supported_formats:
    - "wav"
    - "mp3"
    - "flac"

# Inference settings
inference:
  fallback_model: "example"
  default_timeout: 30.0
  enable_batching: true
  max_batch_size: 8

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file_logging: true
  console_logging: true
