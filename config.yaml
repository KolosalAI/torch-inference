# PyTorch Inference Framework Configuration
# This file contains the main configuration for the inference framework

# Application Configuration
app:
  name: "PyTorch Inference Framework"
  version: "1.0.0"
  description: "High-performance PyTorch model inference with optimization and monitoring"

# Server Configuration - OPTIMIZED FOR HIGH CONCURRENCY
server:
  host: "0.0.0.0"
  port: 8000
  reload: false
  log_level: "INFO"
  workers: 1  # Keep single worker but optimize async handling

# Model Configuration
models:
  default_model: "example"
  model_path: "models/"
  supported_formats:
    - ".pt"
    - ".pth"
    - ".onnx"
    - ".torchscript"
  
  # Model Download Configuration
  download:
    cache_dir: "models/"
    auto_download: true
    sources:
      pytorch_hub:
        enabled: true
      torchvision:
        enabled: true
      huggingface:
        enabled: true
      url:
        enabled: true
        verify_ssl: true
        timeout_seconds: 300
    registry_file: "model_registry.json"

# Device Configuration - OPTIMIZED FOR PERFORMANCE
device:
  type: "auto"  # auto, cpu, cuda, mps
  id: 0  # GPU ID if using CUDA
  use_fp16: true  # Enable FP16 for speed
  use_torch_compile: true  # Enable torch.compile for acceleration
  memory_fraction: 0.9  # Use more GPU memory for better performance

# Batch Processing Configuration - OPTIMIZED FOR ULTRA-LOW LATENCY
batch:
  batch_size: 2  # Optimal batch size based on benchmark
  min_batch_size: 1
  max_batch_size: 8  # Reduced max for better latency
  adaptive_batching: true
  timeout_seconds: 0.05  # Very aggressive timeout (50ms)
  queue_size: 500  # Larger queue for better concurrency

# Preprocessing Configuration
preprocessing:
  input_size:
    width: 224
    height: 224
  normalization:
    mean: [0.485, 0.456, 0.406]
    std: [0.229, 0.224, 0.225]
  interpolation: "bilinear"
  center_crop: true
  normalize: true
  to_rgb: true

# Postprocessing Configuration
postprocessing:
  threshold: 0.5
  nms_threshold: 0.5
  max_detections: 100
  apply_sigmoid: false
  apply_softmax: false

# Performance Configuration - ULTRA-HIGH PERFORMANCE
performance:
  enable_profiling: false  # Disable profiling for production
  enable_metrics: true
  warmup_iterations: 20  # More warmup for stable performance
  benchmark_iterations: 100  # Better benchmarking
  enable_async: true
  max_workers: 12  # Increased workers for better concurrency

# Caching Configuration
cache:
  enable_caching: true
  cache_size: 100
  cache_ttl_seconds: 3600
  disk_cache_path: null

# Audio Configuration
audio:
  sample_rate: 16000
  chunk_duration: 30  # seconds
  overlap: 5          # seconds  
  enable_vad: true    # Voice Activity Detection
  supported_formats: ["wav", "mp3", "flac", "m4a", "ogg"]
  max_audio_length: 300  # seconds
  preprocessing:
    normalize: true
    normalization_method: "peak"  # peak, rms, lufs
    remove_silence: true
    
# TTS Configuration
tts:
  voice: "default"
  speed: 1.0
  pitch: 1.0
  volume: 1.0
  language: "en"
  emotion: null
  output_format: "wav"
  quality: "high"  # low, medium, high
  
# STT Configuration  
stt:
  language: "auto"  # auto, en, es, fr, de, etc.
  enable_timestamps: true
  beam_size: 5
  temperature: 0.0
  suppress_blank: true
  suppress_tokens: [-1]
  initial_prompt: null
  condition_on_previous_text: true

# Security Configuration
security:
  max_file_size_mb: 100
  allowed_extensions:
    - ".jpg"
    - ".jpeg"
    - ".png"
    - ".bmp"
    - ".tiff"
    - ".webp"
    - ".wav"
    - ".mp3"
    - ".flac"
    - ".m4a"
    - ".ogg"
  validate_inputs: true
  sanitize_outputs: true

# Optimization Configuration - ENHANCED FOR SPEED
optimization:
  enable_jit: true
  enable_quantization: true  # Enable quantization for speed
  quantization_mode: "dynamic"  # dynamic, static, qat
  enable_tensorrt: true  # Enable TensorRT if available
  tensorrt_precision: "fp16"  # Use FP16 for TensorRT
  enable_onnx: false
  onnx_providers:
    - "CUDAExecutionProvider"  # Prioritize CUDA
    - "CPUExecutionProvider"

# Monitoring Configuration
monitoring:
  enable_metrics: true
  enable_logging: true
  enable_tracing: false
  metrics:
    port: 9090
    path: "/metrics"
    retention_days: 30
  logging:
    level: "INFO"
    format: "json"
    retention_days: 30
  tracing:
    service_name: "torch-inference"
    sampling_rate: 0.1
    jaeger_endpoint: ""
    zipkin_endpoint: ""

# Enterprise Configuration (optional)
enterprise:
  enabled: false
  auth:
    provider: "jwt"  # jwt, oauth2, saml, ldap
    secret_key: ""
    algorithm: "HS256"
    access_token_expire_minutes: 30
    enable_mfa: false
    enable_api_keys: true
  rbac:
    enable_rbac: false
    default_role: "user"
    admin_users: []
  security:
    enable_encryption_at_rest: false
    enable_rate_limiting: true
    rate_limit_requests_per_minute: 100
    enable_audit_logging: false
  integration:
    database_url: ""
    cache_url: "redis://localhost:6379/0"
    message_broker_url: ""
  scaling:
    enable_auto_scaling: false
    min_replicas: 1
    max_replicas: 10
    cpu_target_utilization: 70

# Environment-specific overrides
environments:
  development:
    server:
      reload: true
      log_level: "DEBUG"
    security:
      validate_inputs: false
    performance:
      enable_profiling: true
    enterprise:
      security:
        enable_rate_limiting: false
  
  staging:
    security:
      validate_inputs: true
    monitoring:
      enable_tracing: true
    enterprise:
      security:
        rate_limit_requests_per_minute: 500
  
  production:
    server:
      workers: 4
    security:
      validate_inputs: true
      sanitize_outputs: true
    enterprise:
      enabled: true
      auth:
        enable_mfa: true
      security:
        enable_encryption_at_rest: true
        enable_audit_logging: true
      scaling:
        enable_auto_scaling: true
